# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1imFoSwZF09gTPFEjnAR6nygHIJLz6UnW
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Sample product comments
comments = [
    "This product exceeded my expectations. I highly recommend it!",
    "The quality of this product is poor. I'm very disappointed.",
    "I love this product! It's perfect for my needs.",
    "Terrible product. Do not waste your money on it.",
    "I am extremely satisfied with this purchase. The product works great.",
    "This product is amazing! It has made my life so much easier.",
    "Not worth the price. The product broke after just a few uses.",
    "I'm not impressed with this product. It doesn't live up to the hype.",
    "Great product! I've been using it for months without any issues.",
    "Avoid this product at all costs. It's a complete waste of money.",
    "Highly disappointed with this product. It didn't work as advertised.",
    "Excellent product! It arrived quickly and in perfect condition.",
    "The product arrived damaged. Very poor quality.",
    "This product is a game-changer. I don't know how I lived without it.",
    "I regret buying this product. It's not worth the money.",
    "The product is okay, but nothing special.",
    "Outstanding product! I've already recommended it to friends and family.",
    "I'm satisfied with this product. It does what it's supposed to do.",
    "This product is a must-have! I use it every day.",
    "Disappointed with this product. It stopped working after a week."
]

# Create a DataFrame
df = pd.DataFrame({'comment': comments})

# Initialize a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the data
X = vectorizer.fit_transform(df['comment'])

# Initialize a KMeans clustering model
num_clusters = 3
kmeans = KMeans(n_clusters=num_clusters, random_state=42)

# Fit the model to the data
kmeans.fit(X)

# Get cluster centers
cluster_centers = kmeans.cluster_centers_

# Assign sentiment scores based on cosine similarity to cluster centers
def get_sentiment_score(comment_vector, cluster_centers):
    similarity_scores = []
    for center in cluster_centers:
        similarity_score = np.dot(comment_vector, center) / (np.linalg.norm(comment_vector) * np.linalg.norm(center))
        similarity_scores.append(similarity_score)
    sentiment_score = max(similarity_scores) * 10  # Scale score to 1-10 range
    return sentiment_score

# Calculate sentiment scores for each comment
sentiment_scores = []
for i in range(X.shape[0]):
    sentiment_score = get_sentiment_score(X[i].toarray(), cluster_centers)
    sentiment_scores.append(sentiment_score)

# Add sentiment scores to DataFrame
df['sentiment_score'] = sentiment_scores

# Print comments along with sentiment scores
print(df)

# Convert sentiment_score column from list to numeric
df['sentiment_score'] = df['sentiment_score'].apply(lambda x: x[0])

# Check data type and content of sentiment_score column
print(df['sentiment_score'].dtype)

# Visualize distribution of sentiment scores
plt.figure(figsize=(10, 6))
sns.histplot(df['sentiment_score'], bins=10, kde=True, color='skyblue')
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

# Sample product comments
comments = [
    "This product exceeded my expectations. I highly recommend it!",
    "The quality of this product is poor. I'm very disappointed.",
    "I love this product! It's perfect for my needs.",
    "Terrible product. Do not waste your money on it.",
    "I am extremely satisfied with this purchase. The product works great.",
    "This product is amazing! It has made my life so much easier.",
    "Not worth the price. The product broke after just a few uses.",
    "I'm not impressed with this product. It doesn't live up to the hype.",
    "Great product! I've been using it for months without any issues.",
    "Avoid this product at all costs. It's a complete waste of money.",
    "Highly disappointed with this product. It didn't work as advertised.",
    "Excellent product! It arrived quickly and in perfect condition.",
    "The product arrived damaged. Very poor quality.",
    "This product is a game-changer. I don't know how I lived without it.",
    "I regret buying this product. It's not worth the money.",
    "The product is okay, but nothing special.",
    "Outstanding product! I've already recommended it to friends and family.",
    "I'm satisfied with this product. It does what it's supposed to do.",
    "This product is a must-have! I use it every day.",
    "Disappointed with this product. It stopped working after a week."
]

# Create a DataFrame
df = pd.DataFrame({'comment': comments})

# Initialize a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the data
X = vectorizer.fit_transform(df['comment'])

# Standardize the data
X = StandardScaler().fit_transform(X.toarray())

# Initialize an Agglomerative Clustering model
agglomerative = AgglomerativeClustering(n_clusters=3)

# Fit the model to the data
agglomerative.fit(X)

# Get cluster labels
labels = agglomerative.labels_

# Initialize a list to store sentiment scores
sentiment_scores = []

# Calculate sentiment scores for each comment
for i in range(X.shape[0]):
    # Noise points are assigned a sentiment score of 0
    if labels[i] == -1:
        sentiment_score = 0
    else:
        # Calculate the average cosine similarity within the cluster
        cluster_indices = np.where(labels == labels[i])[0]
        cluster_center = np.mean(X[cluster_indices], axis=0)
        similarity_scores = cosine_similarity([X[i]], X[cluster_indices])
        sentiment_score = np.mean(similarity_scores) * 10  # Scale score to 1-10 range
    sentiment_scores.append(sentiment_score)

# Add sentiment scores to DataFrame
df['sentiment_score'] = sentiment_scores

# Print comments along with sentiment scores
print(df)

# Visualize distribution of sentiment scores
plt.figure(figsize=(10, 6))
sns.histplot(df['sentiment_score'], bins=10, kde=True, color='skyblue')
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.mixture import GaussianMixture
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

# Sample product comments
comments = [
    "This product exceeded my expectations. I highly recommend it!",
    "The quality of this product is poor. I'm very disappointed.",
    "I love this product! It's perfect for my needs.",
    "Terrible product. Do not waste your money on it.",
    "I am extremely satisfied with this purchase. The product works great.",
    "This product is amazing! It has made my life so much easier.",
    "Not worth the price. The product broke after just a few uses.",
    "I'm not impressed with this product. It doesn't live up to the hype.",
    "Great product! I've been using it for months without any issues.",
    "Avoid this product at all costs. It's a complete waste of money.",
    "Highly disappointed with this product. It didn't work as advertised.",
    "Excellent product! It arrived quickly and in perfect condition.",
    "The product arrived damaged. Very poor quality.",
    "This product is a game-changer. I don't know how I lived without it.",
    "I regret buying this product. It's not worth the money.",
    "The product is okay, but nothing special.",
    "Outstanding product! I've already recommended it to friends and family.",
    "I'm satisfied with this product. It does what it's supposed to do.",
    "This product is a must-have! I use it every day.",
    "Disappointed with this product. It stopped working after a week."
]

# Create a DataFrame
df = pd.DataFrame({'comment': comments})

# Initialize a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the data
X = vectorizer.fit_transform(df['comment'])

# Standardize the data
X = StandardScaler().fit_transform(X.toarray())

# Initialize a Gaussian Mixture Model
gmm = GaussianMixture(n_components=3, random_state=42)

# Fit the model to the data
gmm.fit(X)

# Get cluster labels
labels = gmm.predict(X)

# Get cluster centers
cluster_centers = gmm.means_

# Initialize a list to store sentiment scores
sentiment_scores = []

# Calculate sentiment scores for each comment
for i in range(X.shape[0]):
    # Calculate cosine similarity to the nearest cluster center
    similarity_score = cosine_similarity([X[i]], cluster_centers[labels[i]:labels[i]+1])
    sentiment_score = similarity_score[0][0] * 10  # Scale score to 1-10 range
    sentiment_scores.append(sentiment_score)

# Add sentiment scores to DataFrame
df['sentiment_score'] = sentiment_scores

# Print comments along with sentiment scores
print(df)

# Visualize distribution of sentiment scores
plt.figure(figsize=(10, 6))
sns.histplot(df['sentiment_score'], bins=10, kde=True, color='skyblue')
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import SpectralClustering
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler

# Sample product comments
comments = [
    "This product exceeded my expectations. I highly recommend it!",
    "The quality of this product is poor. I'm very disappointed.",
    "I love this product! It's perfect for my needs.",
    "Terrible product. Do not waste your money on it.",
    "I am extremely satisfied with this purchase. The product works great.",
    "This product is amazing! It has made my life so much easier.",
    "Not worth the price. The product broke after just a few uses.",
    "I'm not impressed with this product. It doesn't live up to the hype.",
    "Great product! I've been using it for months without any issues.",
    "Avoid this product at all costs. It's a complete waste of money.",
    "Highly disappointed with this product. It didn't work as advertised.",
    "Excellent product! It arrived quickly and in perfect condition.",
    "The product arrived damaged. Very poor quality.",
    "This product is a game-changer. I don't know how I lived without it.",
    "I regret buying this product. It's not worth the money.",
    "The product is okay, but nothing special.",
    "Outstanding product! I've already recommended it to friends and family.",
    "I'm satisfied with this product. It does what it's supposed to do.",
    "This product is a must-have! I use it every day.",
    "Disappointed with this product. It stopped working after a week."
]

# Create a DataFrame
df = pd.DataFrame({'comment': comments})

# Initialize a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the data
X = vectorizer.fit_transform(df['comment'])

# Standardize the data
X = StandardScaler().fit_transform(X.toarray())

# Initialize a Spectral Clustering model
spectral = SpectralClustering(n_clusters=3)

# Fit the model to the data
labels = spectral.fit_predict(X)

# Initialize a list to store sentiment scores
sentiment_scores = []

# Calculate sentiment scores for each comment
for i in range(X.shape[0]):
    # Noise points are assigned a sentiment score of 0
    if labels[i] == -1:
        sentiment_score = 0
    else:
        # Calculate the average cosine similarity within the cluster
        cluster_indices = np.where(labels == labels[i])[0]
        cluster_center = np.mean(X[cluster_indices], axis=0)
        similarity_scores = cosine_similarity([X[i]], X[cluster_indices])
        sentiment_score = np.mean(similarity_scores) * 9 + 1  # Scale score to 1-10 range
    sentiment_scores.append(sentiment_score)

# Add sentiment scores to DataFrame
df['sentiment_score'] = sentiment_scores

# Print comments along with sentiment scores
print(df)

# Visualize distribution of sentiment scores
plt.figure(figsize=(10, 6))
sns.histplot(df['sentiment_score'], bins=10, kde=True, color='skyblue')
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import AffinityPropagation
from sklearn.metrics.pairwise import cosine_similarity

# Sample product comments
comments = [
    "This product exceeded my expectations. I highly recommend it!",
    "The quality of this product is poor. I'm very disappointed.",
    "I love this product! It's perfect for my needs.",
    "Terrible product. Do not waste your money on it.",
    "I am extremely satisfied with this purchase. The product works great.",
    "This product is amazing! It has made my life so much easier.",
    "Not worth the price. The product broke after just a few uses.",
    "I'm not impressed with this product. It doesn't live up to the hype.",
    "Great product! I've been using it for months without any issues.",
    "Avoid this product at all costs. It's a complete waste of money.",
    "Highly disappointed with this product. It didn't work as advertised.",
    "Excellent product! It arrived quickly and in perfect condition.",
    "The product arrived damaged. Very poor quality.",
    "This product is a game-changer. I don't know how I lived without it.",
    "I regret buying this product. It's not worth the money.",
    "The product is okay, but nothing special.",
    "Outstanding product! I've already recommended it to friends and family.",
    "I'm satisfied with this product. It does what it's supposed to do.",
    "This product is a must-have! I use it every day.",
    "Disappointed with this product. It stopped working after a week."
]

# Create a DataFrame
df = pd.DataFrame({'comment': comments})

# Initialize a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the data
X = vectorizer.fit_transform(df['comment'])

# Initialize an Affinity Propagation clustering model
affinity_propagation = AffinityPropagation()

# Fit the model to the data
affinity_propagation.fit(X.toarray())

# Get cluster centers
cluster_centers_indices = affinity_propagation.cluster_centers_indices_

# Assign sentiment scores based on cosine similarity to cluster centers
def get_sentiment_score(comment_vector, cluster_centers):
    similarity_scores = []
    for center in cluster_centers:
        similarity_score = np.dot(comment_vector, center) / (np.linalg.norm(comment_vector) * np.linalg.norm(center))
        similarity_scores.append(similarity_score)
    sentiment_score = max(similarity_scores) * 10  # Scale score to 1-10 range
    return sentiment_score

# Calculate sentiment scores for each comment
sentiment_scores = []
for i in range(X.shape[0]):
    sentiment_score = get_sentiment_score(X[i].toarray(), X[cluster_centers_indices].toarray())
    sentiment_scores.append(sentiment_score)

# Add sentiment scores to DataFrame
df['sentiment_score'] = sentiment_scores

# Print comments along with sentiment scores
print(df)

# Convert sentiment_score column from list to numeric
df['sentiment_score'] = df['sentiment_score'].apply(lambda x: x[0])

# Check data type and content of sentiment_score column
print(df['sentiment_score'].dtype)

# Visualize distribution of sentiment scores
plt.figure(figsize=(10, 6))
sns.histplot(df['sentiment_score'], bins=10, kde=True, color='skyblue')
plt.title('Distribution of Sentiment Scores')
plt.xlabel('Sentiment Score')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering,AffinityPropagation
from sklearn.mixture import GaussianMixture
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score

# Sample product comments
comments = [
    "This product exceeded my expectations. I highly recommend it!",
    "The quality of this product is poor. I'm very disappointed.",
    "I love this product! It's perfect for my needs.",
    "Terrible product. Do not waste your money on it.",
    "I am extremely satisfied with this purchase. The product works great.",
    "This product is amazing! It has made my life so much easier.",
    "Not worth the price. The product broke after just a few uses.",
    "I'm not impressed with this product. It doesn't live up to the hype.",
    "Great product! I've been using it for months without any issues.",
    "Avoid this product at all costs. It's a complete waste of money.",
    "Highly disappointed with this product. It didn't work as advertised.",
    "Excellent product! It arrived quickly and in perfect condition.",
    "The product arrived damaged. Very poor quality.",
    "This product is a game-changer. I don't know how I lived without it.",
    "I regret buying this product. It's not worth the money.",
    "The product is okay, but nothing special.",
    "Outstanding product! I've already recommended it to friends and family.",
    "I'm satisfied with this product. It does what it's supposed to do.",
    "This product is a must-have! I use it every day.",
    "Disappointed with this product. It stopped working after a week."
]

# Create a DataFrame
df = pd.DataFrame({'comment': comments})

# Initialize a TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')

# Fit and transform the data
X = vectorizer.fit_transform(df['comment'])

# Standardize the data
X = StandardScaler().fit_transform(X.toarray())

# Split data into training and testing sets
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)

# Define a dictionary to store evaluation results
evaluation_results = {}

# Define a list of clustering algorithms to compare
algorithms = ['KMeans', 'Agglomerative Clustering', 'Gaussian Mixture', 'Spectral Clustering', 'AffinityPropagation']

# Define the evaluation metric
evaluation_metric = 'silhouette'  # You can change this to any other metric like 'davies_bouldin' or 'adjusted_rand'

# Iterate over algorithms
for algorithm in algorithms:
    # Initialize the clustering model with different parameters
    if algorithm == 'KMeans':
        model = KMeans(n_clusters=3, random_state=42)
    elif algorithm == 'Agglomerative Clustering':
        model = AgglomerativeClustering(n_clusters=3)
    elif algorithm == 'Gaussian Mixture':
        model = GaussianMixture(n_components=3, random_state=42)
    elif algorithm == 'Spectral Clustering':
        model = SpectralClustering(n_clusters=3)
    elif algorithm == 'AffinityPropagation':
        model = AffinityPropagation()

    # Fit the model to the training data
    model.fit(X_train)

    # Predict clusters for the testing data
    labels = model.fit_predict(X_test)

    # Calculate evaluation metric
    if evaluation_metric == 'silhouette':
        score = silhouette_score(X_test, labels)
    elif evaluation_metric == 'davies_bouldin':
        score = davies_bouldin_score(X_test, labels)
    elif evaluation_metric == 'adjusted_rand':
        # Adjusted Rand Index requires true labels, which may not be available in unsupervised learning
        # Adjust the code accordingly if you have true labels
        score = adjusted_rand_score(true_labels, labels)

    # Store the evaluation result
    evaluation_results[algorithm] = score
y=list(evaluation_results.values())
print(y)
# Plot the accuracies
plt.figure(figsize=(10, 6))
sns.barplot(x=list(evaluation_results.keys()), y=list(evaluation_results.values()))
plt.title('Comparison of Clustering Algorithm Accuracies')
plt.xlabel('Clustering Algorithm')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

# Find the algorithm with the highest accuracy
best_algorithm = max(evaluation_results, key=evaluation_results.get)
best_accuracy = evaluation_results[best_algorithm]

print("Best Clustering Algorithm:", best_algorithm)
print("Best Accuracy:", best_accuracy)